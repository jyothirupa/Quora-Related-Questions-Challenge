{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import textblob as txb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from time import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. The user enters a question\\n2. Preprocess it and extract the features of the question\\n3. extract the tokenized features of it and save it for optimized comparision\\n4. construct the reduced set of questions similar to the features of the input questions\\n5. perform jaccard sim among the union of the reduced set \\n6. threshold to give related questions\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#workflow\n",
    "'''\n",
    "1. The user enters a question\n",
    "2. Preprocess it and extract the features of the question\n",
    "3. extract the tokenized features of it and save it for optimized comparision\n",
    "4. construct the reduced set of questions similar to the features of the input questions\n",
    "5. perform jaccard sim among the union of the reduced set \n",
    "6. threshold to give related questions\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global elements that need to be booted up when the software starts\n",
    "config = {\n",
    "    'ps' : nltk.PorterStemmer(),\n",
    "    'inp_q_features' : None,\n",
    "    'inp_q_tokens' : None,\n",
    "    'inp_q' : None, \n",
    "    'helping_word' : ['am', 'are', 'is', 'was', 'were', 'be', 'being', 'been','have', 'has', 'had', 'shall', 'will','do', 'does', 'did', 'may', 'must', 'might', 'can', 'could', 'would', 'should', 'i'],\n",
    "    'clusters' : None,\n",
    "    'len_nulls' : None,\n",
    "    'largest_cluster' : None,\n",
    "    'smallest_cluster' : None\n",
    "}\n",
    "\n",
    "def preprocessing(sentence):\n",
    "    review_text = sentence.lower()\n",
    "    #review_text = re.sub(r\"[A-Za-z0-9]\", \" \", review_text)\n",
    "    review_text = re.sub(r\"[^A-Za-z0-9(),!.?\\'\\`\\\"]\", \" \", review_text)\n",
    "    review_text = re.sub(r\"\\'s\", \" \", review_text)\n",
    "    review_text = re.sub(r\"\\'ve\", \" \", review_text)\n",
    "    review_text = re.sub(r\"n\\'t\", \" \", review_text)\n",
    "    review_text = re.sub(r\"\\'re\", \" \", review_text)\n",
    "    review_text = re.sub(r\"\\'d\", \" \", review_text)\n",
    "    review_text = re.sub(r\"\\'ll\", \" \", review_text)\n",
    "    review_text = re.sub(r\",\", \" \", review_text)\n",
    "    review_text = re.sub(r\"\\.\", \" \", review_text)\n",
    "    review_text = re.sub(r\"!\", \" \", review_text)\n",
    "    review_text = re.sub(r\"\\(\", \" ( \", review_text)\n",
    "    review_text = re.sub(r\"\\)\", \" ) \", review_text)\n",
    "    review_text = re.sub(r\"\\?\", \" \", review_text)\n",
    "    review_text = re.sub(r\"\\'\", \"\", review_text)\n",
    "    review_text = re.sub(r\"\\s{2,}\", \" \", review_text)\n",
    "    return review_text\n",
    "\n",
    "def get_nouns(text):\n",
    "    ps = config['ps']\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    \n",
    "    #getting all the nouns\n",
    "    nouns = [i for i in tagged if i[1][0]=='N']\n",
    "    f_nouns = []\n",
    "    for i in nouns:\n",
    "        '''if(i[1]=='NNS'):#if it is common noun in plural form then get its stemmed word\n",
    "            f_nouns.append(ps.stem(i[0]))\n",
    "        else:\n",
    "            f_nouns.append(i[0].lower())'''\n",
    "        f_nouns.append(ps.stem(i[0]))\n",
    "    return set(f_nouns)\n",
    "\n",
    "def get_noun_phrases(sentence):\n",
    "    wiki = txb.TextBlob(sentence)\n",
    "    noun_phrases = [str(i) for i in wiki.noun_phrases]\n",
    "    return set(noun_phrases)\n",
    "\n",
    "def get_verbs_with_addons(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    return set([i[0] for i in tagged if i[1][0] in ('R','V') and i[0] not in config['helping_word']])\n",
    "\n",
    "def get_features(sentence):\n",
    "    #removing redundancy by eliminating all the nouns which are already present in the noun phrase\n",
    "    text = preprocessing(sentence)\n",
    "    \n",
    "    nouns = get_nouns(text)\n",
    "    noun_phrases = get_noun_phrases(text)\n",
    "    verbs_adverbs = get_verbs_with_addons(text)\n",
    "    \n",
    "    #if both the sets are empty then return a null set else if one is empty then return other\n",
    "    if((len(nouns)==0) and (len(noun_phrases)==0)):\n",
    "        return verbs_adverbs\n",
    "    if(len(nouns)==0):\n",
    "        return noun_phrases.union(verbs_adverbs)\n",
    "    if(len(noun_phrases)==0):\n",
    "        return nouns.union(verbs_adverbs)\n",
    "    #if both non empty then select just the uniques\n",
    "    nn = [[i,0] for i in nouns]\n",
    "    for np in noun_phrases:\n",
    "        for n in nn:\n",
    "            if(n[0] in np):\n",
    "                n[1]=1\n",
    "    x=[]\n",
    "    for i in nn:\n",
    "        if(i[1]==0):\n",
    "            x.append(i[0])\n",
    "\n",
    "    x = [i[0] for i in nn if i[1]==0]\n",
    "    return set(x+list(noun_phrases)+list(verbs_adverbs))\n",
    "\n",
    "def construct_questions_pool(dataset):\n",
    "    questions_pool = []\n",
    "    for i in dataset.id:\n",
    "        if(dataset.loc[i,'is_duplicate']==1):\n",
    "            questions_pool.append(preprocessing(dataset.loc[i,'question1']))\n",
    "        else:\n",
    "            questions_pool.append(preprocessing(dataset.loc[i,'question1']))\n",
    "            questions_pool.append(preprocessing(dataset.loc[i,'question2']))\n",
    "    return set(questions_pool)\n",
    "\n",
    "#this function saves just the indexes of the questions in the clusters for us retrieve later\n",
    "def build_indexed_clusters(dataset):\n",
    "    clusters = dict()\n",
    "    nulls = list()\n",
    "    \n",
    "    questions = dataset.question\n",
    "    indexes = dataset.index\n",
    "    for i in indexes:\n",
    "        features = get_features(questions[i])    \n",
    "        if(len(features)!=0):\n",
    "            for j in features:\n",
    "                if(j in clusters.keys()):\n",
    "                    clusters[j].append(i)\n",
    "                else:\n",
    "                    clusters[j]=list()\n",
    "                    clusters[j].append(i)\n",
    "        else:\n",
    "            nulls.append(i)\n",
    "    \n",
    "    clusters['-1']=nulls\n",
    "    return clusters\n",
    "\n",
    "def build_dataset_with_features(questions):\n",
    "    ind = []\n",
    "    features = []\n",
    "    questions = list(questions)\n",
    "    for i in range(len(questions)):\n",
    "        ind.append(i)\n",
    "        features.append(tokenized_features(questions[i]))\n",
    "    \n",
    "    return pd.DataFrame({'index' : ind,\n",
    "                        'question' : questions,\n",
    "                        'features' : features})\n",
    "\n",
    "def save_dataset(dataset):\n",
    "    dataset.to_csv('featured_questions.csv', sep=',', encoding='utf-8', index=False)\n",
    "    \n",
    "def save_clusters(dataset):\n",
    "    obj = build_indexed_clusters(dataset)\n",
    "    with open('clusters.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def save_word_index(m_dict):\n",
    "    with open('word_index.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def load_clusters(name='clusters.pkl'):\n",
    "    with open(name, 'rb') as f:\n",
    "        clusters = pickle.load(f)\n",
    "        config['clusters'] = sum([1 for i in clusters.keys()])\n",
    "        config['len_nulls'] = len(clusters['-1'])\n",
    "        tmp = [len(clusters[i]) for i in clusters.keys()]\n",
    "        config['largest_cluster'] = max(tmp)\n",
    "        config['smallest_cluster'] = min(tmp)\n",
    "        return clusters\n",
    "\n",
    "#returns True if there are any features of the input question\n",
    "#returns False if there are no features of the input question\n",
    "def init_question(question):\n",
    "    question = preprocessing(question)\n",
    "    config['inp_q'] = question\n",
    "    config['inp_q_features'] = get_features(question)\n",
    "    config['inp_q_tokens'] = tokenized_features(question)\n",
    "    if(config['inp_q_features'] is None):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "#construct a reduced set of questions by taking the union of the clusters of the keywords present in the input questions\n",
    "def construct_reduced_question_pool(clusters, state):\n",
    "    if(state==True):\n",
    "        q_tokens = list(config['inp_q_features'])\n",
    "        f_list = []\n",
    "        for i in q_tokens:\n",
    "            try:\n",
    "                f_list = f_list + clusters[i]\n",
    "            except:\n",
    "                print('Error : No clusters found having for the keyword ',i)\n",
    "        f_list = list(set(f_list))\n",
    "        return f_list\n",
    "    else:\n",
    "        #if the input question did not give us features then check for it in the common null pool\n",
    "        return list(clusters['-1'])\n",
    "    \n",
    "#this function returns the best 5 matching questions from among the dataset\n",
    "def rank(indexed_features):\n",
    "    q_tokens = config['inp_q_tokens']\n",
    "    results = list()\n",
    "    for i in indexed_features.index:\n",
    "        sim_ind = jaccard(q_tokens, indexed_features.loc[i,'features'])\n",
    "        results.append({'sim_index':sim_ind,'question':indexed_features.loc[i,'question']})\n",
    "    results.sort(key = lambda x:x['sim_index'], reverse=True)\n",
    "    #return results[0:5]\n",
    "    return [i['question'] for i in results][0:20]\n",
    "    \n",
    "def tokenized_features(inp_ques):\n",
    "    t_features = []\n",
    "    features = get_features(inp_ques)\n",
    "    for i in features:\n",
    "        t_features = t_features + [j for j in i.split()]\n",
    "    return set(t_features)\n",
    "\n",
    "def jaccard(tokens1, tokens2):\n",
    "    #receives the set of tokens1 and token2\n",
    "    inter = tokens1.intersection(tokens2)\n",
    "    uni = tokens1.union(tokens2)\n",
    "    return float(len(inter)/len(uni))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the dataset train.csv to get the questions\n",
    "file_path = 'C:\\\\Users\\\\Administrator\\\\Related questions\\\\Dataset\\\\train.csv'\n",
    "\n",
    "#doing all the initializing stuff\n",
    "start_time = time()\n",
    "\n",
    "df = pd.read_csv(file_path, encoding = 'utf8')\n",
    "df = df.dropna()\n",
    "#remove this sampling when doing for the final time\n",
    "#df = df.sample(n=540)\n",
    "\n",
    "questions = construct_questions_pool(df)\n",
    "featured_dataset = build_dataset_with_features(questions)\n",
    "\n",
    "save_dataset(featured_dataset)\n",
    "save_clusters(featured_dataset)\n",
    "\n",
    "end_time = time()\n",
    "print('Time taken for creating the clusters and saving things : ', (end_time - start_time), 'secs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for initialization :  2.2512102127075195 secs\n"
     ]
    }
   ],
   "source": [
    "#loading up the clusters and featured dataset and performing the operations\n",
    "\n",
    "start_time = time()\n",
    "clusters = load_clusters()\n",
    "df = pd.read_csv('featured_questions.csv')\n",
    "end_time = time()\n",
    "print('Time taken for initialization : ', (end_time - start_time), 'secs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the question : What is something that never fails to make you happy?\n"
     ]
    }
   ],
   "source": [
    "#final execution script for the execution flow\n",
    "question = input('Enter the question : ')\n",
    "val = init_question(question)\n",
    "questions_list = construct_reduced_question_pool(clusters, val)\n",
    "reduced_with_features = df.iloc[[i for i in questions_list],:]\n",
    "result = rank(reduced_with_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'question' (str)\n"
     ]
    }
   ],
   "source": [
    "%store question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how can i make money utilising polyvore ',\n",
       " 'why do some quora questions never get answers even when they get views ',\n",
       " 'how can i send a text message and make it look like it came from someone else phone number ',\n",
       " 'how do i urge a girl to kiss me what are some body gestures required to seduce her or something like that ',\n",
       " 'what can i do add to make oatmeal less bland tasting ',\n",
       " 'what is one band you have never gotten bored of ',\n",
       " 'are my parents abusive i grew up on imaginary friends because i never knew there was an outside world i try to impress them they were never there ',\n",
       " 'how can i make money on whatsapp or by whatsapp ',\n",
       " 'using a demand and supply diagram analyse why a fall in incomes may reduce the market price of houses how would i make this graph ',\n",
       " 'relationship advice how do you know if you in love or obsessed with a guy if you never met him in person but feel like you have known him for ages ',\n",
       " 'what are the best strategies to make money trading binary options online ',\n",
       " 'can i make 900k a year playing poker online why or why not ',\n",
       " 'how is it that the hubble space telescope can see something 13 4 billion light years away but ca see the flag on the moon ',\n",
       " 'what is the best free music software to make 8 bit music ',\n",
       " 'what is more watery flow release or airbrush if airbrush how much of that can i use to thin my heavy paints and make them fluid ',\n",
       " 'can i make a solar dryer locally ',\n",
       " 'what are some extracurricular activities that can make a negative impression on college admissions officers ',\n",
       " 'is it illegal to provoke something in order for business to take place ( e g getting a car dirty so the car owner is \"forced\" to visit the car wash ) is there proper terminology for this in business ',\n",
       " 'how do you get over someone you never met but were intimate with ',\n",
       " 'do dumbo rats make good pets why or why not ']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'result' (list)\n"
     ]
    }
   ],
   "source": [
    "%store result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
